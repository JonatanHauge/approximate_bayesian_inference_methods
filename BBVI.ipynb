{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5 NN for image classfication on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "\n",
    "- Vi har trænet et LeNet5 netværk til at classificere MNIST cifre, lad os kalde det for NN.\n",
    "\n",
    "$$ NN:~\\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10} $$\n",
    "\n",
    "- Til at lave predictions bruger vi softmax, $\\pi(x)$, på outputtet af netværket og får dermed en sandsynlighedsvektor ud.\n",
    "\n",
    "Dermed bliver \n",
    "\n",
    "$$ P(y_n|\\theta) = Cat(y_n|\\pi(NN(x_n))) = \\prod_{i=1}^{10} \\pi_{i}(f(x_n))^{y_{n,i}} \\in [0,1] $$\n",
    "\n",
    "Og\n",
    "\n",
    "$$ P(y|NN) = \\prod_{n=1}^N Cat(y_n|\\pi (NN(x_n))) $$\n",
    "\n",
    "Vi ønsker at lave BBVI i dette setup, men på et underrum da vi antager at antal parametre/vægte er for mange til at dette kan beregnes på rimelig tid.\n",
    "\n",
    "Vi ønsker at bruge følgende variational family (mean field gaussians):\n",
    "\n",
    "$$ Q =\\{ N(m,V)|m\\in \\mathbb{R}^K, V\\in \\mathbb{M}^{Diag~KxK},~v_i>0\\} $$\n",
    "\n",
    "ELBO ser da således ud:\n",
    "\n",
    "$$ L(q) = \\mathbb{E}_{q(z)}[log~P(y,z)] - \\mathbb{E}_{q(z)}[log~q(z)], z\\in\\mathbb{R}^K $$\n",
    "\n",
    "Sidste led (entropien) kan regnes analytisk (Vist i aflevering 3):\n",
    "\n",
    "$$ H(q) = - \\mathbb{E}_{q(z)}[log~q(z)] = \\frac{1}{2} \\sum_{i=1}^K log(2e\\pi v_i) $$\n",
    "\n",
    "Første led regnes med MC estimater af $z^s$\n",
    "\n",
    "$$ \\mathbb{E}_{q(z)[log~P(y,z)]} \\approx \\frac{1}{S} \\sum_{s=1}^S \\sum_{n=1}^N log~P(y_n|z^s) + \\frac{1}{S}\\sum_{s=1}^S log~P(z^s), $$\n",
    "\n",
    "hvor \n",
    "\n",
    "$$ P(y_n|z^s) \\sim Cat(y_n| \\pi(NN_{\\theta^s}(x_n))), ~~\\theta^s=\\theta_{MAP} + Pz $$\n",
    "\n",
    "og $$ NN_{\\theta^s} $$ angiver det neurale netværk med $\\theta^s$ som vægte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from utils import AdamOptimizer, VariationalInference, BlackBoxVariationalInference\n",
    "import autograd.numpy as np\n",
    "import torch \n",
    "from torchvision.datasets import MNIST\n",
    "from models.LeNet5 import LeNet\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist_train = MNIST('./datasets', train=True, download=True)\n",
    "mnist_test = MNIST('./datasets', train=False, download=True)\n",
    "\n",
    "# load the data\n",
    "xtrain = mnist_train.train_data\n",
    "ytrain = mnist_train.train_labels\n",
    "xtest = mnist_test.test_data\n",
    "ytest = mnist_test.test_labels\n",
    "\n",
    "# normalize the data\n",
    "xtrain = xtrain.float()/255\n",
    "xtest = xtest.float()/255\n",
    "\n",
    "#insert a channel dimension\n",
    "xtrain = xtrain.unsqueeze(1)\n",
    "xtest = xtest.unsqueeze(1)\n",
    "\n",
    "#print shapes\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*np.log(2*np.pi*v)\n",
    "log_mvnpdf = lambda x, m, v: -0.5*np.sum((x-m)**2/v + np.log(2*np.pi*v), axis=1)\n",
    "softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]\n",
    "\n",
    "def new_weights_in_NN(model, new_weight_vector):\n",
    "    current_index = 0\n",
    "    # Iterate over each parameter in the model\n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel() # number of elements in the tensor\n",
    "        new_weights = new_weight_vector[current_index:current_index + num_params].view_as(param.data) # reshape the new weights to the shape of the parameter tensor\n",
    "        param.data.copy_(new_weights) # copy the new weights to the parameter tensor\n",
    "        current_index += num_params # update the current index\n",
    "\n",
    "    return model\n",
    "\n",
    "def set_weights(model, vector):\n",
    "    offset = 0\n",
    "    for param in model.parameters():\n",
    "        param.data.copy_(vector[offset:offset + param.numel()].view(param.size()))\n",
    "        offset += param.numel()\n",
    "\n",
    "def log_prior_pdf(z, prior_mean, prior_var):\n",
    "    \"\"\" Evaluates the log prior Gaussian for each sample of z. \n",
    "        D denote the dimensionality of the model and S denotes the number of MC samples.\n",
    "\n",
    "        Inputs:\n",
    "            z             -- np.array of shape (S, 2*K)\n",
    "            prior_mean    -- np.array of shape (S, K)\n",
    "            prior_var     -- np.array of shape (S, K)\n",
    "\n",
    "        Returns:\n",
    "            log_prior     -- np.array of shape (1,)???\n",
    "       \"\"\"\n",
    "    log_prior = np.sum(log_npdf(z, prior_mean, prior_var), axis=1)\n",
    "    return log_prior\n",
    "\n",
    "def log_like_NN_classification(X, y, theta_s):\n",
    "    \"\"\"\n",
    "    Implements the log likelihood function for the classification NN with categorical likelihood.\n",
    "    S is number of MC samples, N is number of datapoints in likelihood and D is the dimensionality of the model (number of weights).\n",
    "\n",
    "    Inputs:\n",
    "    X              -- Data (np.array of size N x D)\n",
    "    y              -- vector of target (np.array of size N)\n",
    "    theta_s        -- vector of weights (np.array of size (S, D))\n",
    "\n",
    "    outputs: \n",
    "    log_likelihood -- Array of log likelihood for each sample in z (np.array of size S)\n",
    "     \"\"\"\n",
    "    S = theta_s.shape[0]\n",
    "    #net = LeNet()\n",
    "    log_likelihood = 0\n",
    "    for i in range(S):\n",
    "        net_s = set_weights(net, torch.tensor(theta_s[i]).float())\n",
    "        outputs = softmax(net_s(X).detach().numpy())\n",
    "        #categorical log likelihood\n",
    "        log_likelihood += np.sum(np.log(outputs[np.arange(len(y)), y]))\n",
    "    \n",
    "    return log_likelihood / S\n",
    "\n",
    "#class BlackBoxVariationalInference(VariationalInference):\n",
    "#    def __init__(self, theta_map, P, log_prior, log_lik, num_params, step_size=1e-2, max_itt=2000, num_samples=20, batch_size=None, seed=0, verbose=False):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 61706\n",
      "Fitting approximation using Black-box VI\n",
      "shapes: (20, 10) (10, 61706) (61706,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m bbvi \u001b[38;5;241m=\u001b[39m BlackBoxVariationalInference(theta_map, P, log_prior_pdf, log_like_NN_classification, K, step_size, max_itt, num_samples, batch_size, seed, verbose)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mbbvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johau\\Desktop\\Approximate inference methods for BNNs\\approximate_bayesian_inference_methods\\utils.py:109\u001b[0m, in \u001b[0;36mVariationalInference.fit\u001b[1;34m(self, X, y, seed)\u001b[0m\n\u001b[0;32m    105\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_itt):\n\u001b[0;32m    107\u001b[0m     \n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# evaluate ELBO\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELBO \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_ELBO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# store current values for plotting purposes\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELBO_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELBO)\n",
      "File \u001b[1;32mc:\\Users\\johau\\Desktop\\Approximate inference methods for BNNs\\approximate_bayesian_inference_methods\\utils.py:190\u001b[0m, in \u001b[0;36mBlackBoxVariationalInference.compute_ELBO\u001b[1;34m(self, lam)\u001b[0m\n\u001b[0;32m    187\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[batch_idx, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[batch_idx]\n\u001b[1;32m--> 190\u001b[0m     expected_log_lik_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_lik\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_samples\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: scalar\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# No mini-batching\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     expected_log_lik_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_lik(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, w_samples)   \u001b[38;5;66;03m# shape: scalar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[72], line 55\u001b[0m, in \u001b[0;36mlog_like_NN_classification\u001b[1;34m(X, y, theta_s)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(S):\n\u001b[0;32m     54\u001b[0m     net_s \u001b[38;5;241m=\u001b[39m set_weights(net, torch\u001b[38;5;241m.\u001b[39mtensor(theta_s[i])\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 55\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(\u001b[43mnet_s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m#categorical log likelihood\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     log_likelihood \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(outputs[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y)), y]))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "#load weights\n",
    "weights = torch.load('checkpoints\\LeNet5_acc_95.12%.pth')\n",
    "theta_map = torch.cat([w.flatten() for w in weights.values()]) # flatten the weights\n",
    "theta_map = theta_map.detach().numpy()\n",
    "\n",
    "# settings\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print('Number of parameters:', num_params)\n",
    "K = 10\n",
    "P = torch.randn(num_params, K).numpy() # random matrix from normal distribution\n",
    "max_itt = 10\n",
    "step_size = 5e-2\n",
    "num_samples = 20\n",
    "batch_size = 5\n",
    "seed = 0\n",
    "verbose = True\n",
    "\n",
    "bbvi = BlackBoxVariationalInference(theta_map, P, log_prior_pdf, log_like_NN_classification, K, step_size, max_itt, num_samples, batch_size, seed, verbose)\n",
    "bbvi.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 0 matches.\n",
      "Parameter 1 matches.\n",
      "Parameter 2 matches.\n",
      "Parameter 3 matches.\n",
      "Parameter 4 matches.\n",
      "Parameter 5 matches.\n",
      "Parameter 6 matches.\n",
      "Parameter 7 matches.\n",
      "Parameter 8 matches.\n",
      "Parameter 9 matches.\n"
     ]
    }
   ],
   "source": [
    "def set_weights(model, vector):\n",
    "    offset = 0\n",
    "    for param in model.parameters():\n",
    "        param.data.copy_(vector[offset:offset + param.numel()].view(param.size()))\n",
    "        offset += param.numel()\n",
    "\n",
    "net = LeNet()\n",
    "weights = torch.load('checkpoints\\LeNet5_acc_95.12%.pth')\n",
    "theta_map = torch.cat([w.flatten() for w in weights.values()]) # flatten the weights\n",
    "set_weights(net, theta_map)\n",
    "\n",
    "\n",
    "model_params = list(net.state_dict().values())  # Get the parameters of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
