{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5 NN for image classfication on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "\n",
    "- Vi har trænet et LeNet5 netværk til at classificere MNIST cifre, lad os kalde det for NN.\n",
    "\n",
    "$$ NN:~\\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10} $$\n",
    "\n",
    "- Til at lave predictions bruger vi softmax, $\\pi(x)$, på outputtet af netværket og får dermed en sandsynlighedsvektor ud.\n",
    "\n",
    "Dermed bliver \n",
    "\n",
    "$$ P(y_n|\\theta) = Cat(y_n|\\pi(NN(x_n))) = \\prod_{i=1}^{10} \\pi_{i}(f(x_n))^{y_{n,i}} \\in [0,1] $$\n",
    "\n",
    "Og\n",
    "\n",
    "$$ P(y|NN) = \\prod_{n=1}^N Cat(y_n|\\pi (NN(x_n))) $$\n",
    "\n",
    "Vi ønsker at lave BBVI i dette setup, men på et underrum da vi antager at antal parametre/vægte er for mange til at dette kan beregnes på rimelig tid.\n",
    "\n",
    "Vi ønsker at bruge følgende variational family (mean field gaussians):\n",
    "\n",
    "$$ Q =\\{ N(m,V)|m\\in \\mathbb{R}^K, V\\in \\mathbb{M}^{Diag~KxK},~v_i>0\\} $$\n",
    "\n",
    "ELBO ser da således ud:\n",
    "\n",
    "$$ L(q) = \\mathbb{E}_{q(z)}[log~P(y,z)] - \\mathbb{E}_{q(z)}[log~q(z)], z\\in\\mathbb{R}^K $$\n",
    "\n",
    "Sidste led (entropien) kan regnes analytisk (Vist i aflevering 3):\n",
    "\n",
    "$$ H(q) = - \\mathbb{E}_{q(z)}[log~q(z)] = \\frac{1}{2} \\sum_{i=1}^K log(2e\\pi v_i) $$\n",
    "\n",
    "Første led regnes med MC estimater af $z^s$\n",
    "\n",
    "$$ \\mathbb{E}_{q(z)[log~P(y,z)]} \\approx \\frac{1}{S} \\sum_{s=1}^S \\sum_{n=1}^N log~P(y_n|z^s) + \\frac{1}{S}\\sum_{s=1}^S log~P(z^s), $$\n",
    "\n",
    "hvor \n",
    "\n",
    "$$ P(y_n|z^s) \\sim Cat(y_n| \\pi(NN_{\\theta^s}(x_n))), ~~\\theta^s=\\theta_{MAP} + Pz $$\n",
    "\n",
    "og $$ NN_{\\theta^s} $$ angiver det neurale netværk med $\\theta^s$ som vægte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "c:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "c:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "c:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "from utils import AdamOptimizer, VariationalInference, BlackBoxVariationalInference\n",
    "import autograd.numpy as np\n",
    "import torch \n",
    "from torchvision.datasets import MNIST\n",
    "from models.LeNet5 import LeNet\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist_train = MNIST('./datasets', train=True, download=True)\n",
    "mnist_test = MNIST('./datasets', train=False, download=True)\n",
    "\n",
    "# load the data\n",
    "xtrain = mnist_train.train_data\n",
    "ytrain = mnist_train.train_labels\n",
    "xtest = mnist_test.test_data\n",
    "ytest = mnist_test.test_labels\n",
    "\n",
    "# normalize the data\n",
    "xtrain = xtrain.float()/255\n",
    "xtest = xtest.float()/255\n",
    "\n",
    "#insert a channel dimension\n",
    "xtrain = xtrain.unsqueeze(1)\n",
    "xtest = xtest.unsqueeze(1)\n",
    "\n",
    "#print shapes\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_npdf = lambda x, m, v: -(x-m)**2/(2*v) - 0.5*np.log(2*np.pi*v)\n",
    "log_mvnpdf = lambda x, m, v: -0.5*np.sum((x-m)**2/v + np.log(2*np.pi*v), axis=1)\n",
    "softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]\n",
    "\n",
    "def new_weights_in_NN(model, new_weight_vector):\n",
    "    current_index = 0\n",
    "    # Iterate over each parameter in the model\n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel() # number of elements in the tensor\n",
    "        new_weights = new_weight_vector[current_index:current_index + num_params].view_as(param.data) # reshape the new weights to the shape of the parameter tensor\n",
    "        param.data.copy_(new_weights) # copy the new weights to the parameter tensor\n",
    "        current_index += num_params # update the current index\n",
    "\n",
    "    return model\n",
    "\n",
    "def log_prior_pdf(z, prior_mean, prior_var):\n",
    "    \"\"\" Evaluates the log prior Gaussian for each sample of z. \n",
    "        D denote the dimensionality of the model and S denotes the number of MC samples.\n",
    "\n",
    "        Inputs:\n",
    "            z             -- np.array of shape (S, 2*K)\n",
    "            prior_mean    -- np.array of shape (S, K)\n",
    "            prior_var     -- np.array of shape (S, K)\n",
    "\n",
    "        Returns:\n",
    "            log_prior     -- np.array of shape (1,)???\n",
    "       \"\"\"\n",
    "    log_prior = np.sum(log_npdf(z, prior_mean, prior_var), axis=1)\n",
    "    return log_prior\n",
    "\n",
    "def log_like_NN_classification(X, y, theta_s):\n",
    "    \"\"\"\n",
    "    Implements the log likelihood function for the classification NN with categorical likelihood.\n",
    "    S is number of MC samples, N is number of datapoints in likelihood and D is the dimensionality of the model (number of weights).\n",
    "\n",
    "    Inputs:\n",
    "    X              -- Data (np.array of size N x D)\n",
    "    y              -- vector of target (np.array of size N)\n",
    "    theta_s        -- vector of weights (np.array of size (S, D))\n",
    "\n",
    "    outputs: \n",
    "    log_likelihood -- Array of log likelihood for each sample in z (np.array of size S)\n",
    "     \"\"\"\n",
    "    S = theta_s.shape[0]\n",
    "    #net = LeNet()\n",
    "    log_likelihood = 0\n",
    "    for i in range(S):\n",
    "        print(\"Shape of theta_s[i]:\", theta_s[i].shape)\n",
    "        net_s = new_weights_in_NN(net, torch.tensor(theta_s[i]).float())\n",
    "        outputs = softmax(net_s(X).detach().numpy())\n",
    "        #categorical log likelihood\n",
    "        log_likelihood += np.sum(np.log(outputs[np.arange(len(y)), y]))\n",
    "    \n",
    "    return log_likelihood / S\n",
    "\n",
    "#class BlackBoxVariationalInference(VariationalInference):\n",
    "#    def __init__(self, theta_map, P, log_prior, log_lik, num_params, step_size=1e-2, max_itt=2000, num_samples=20, batch_size=None, seed=0, verbose=False):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 61706\n",
      "Fitting approximation using Black-box VI\n",
      "shapes: (20, 10) (10, 61706) (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n",
      "shapes: (20, 10) (10, 61706) (61706,)\n",
      "Shape of theta_s[i]: (61706,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johau\\AppData\\Local\\Temp\\ipykernel_18724\\766712419.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m bbvi \u001b[38;5;241m=\u001b[39m BlackBoxVariationalInference(theta_map, P, log_prior_pdf, log_like_NN_classification, K, step_size, max_itt, num_samples, batch_size, seed, verbose)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mbbvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johau\\Desktop\\Approximate inference methods for BNNs\\approximate_bayesian_inference_methods\\utils.py:116\u001b[0m, in \u001b[0;36mVariationalInference.fit\u001b[1;34m(self, X, y, seed)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# compute gradient of ELBO wrt. variational parameters\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_ELBO_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# take gradient step\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(g)\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[38;5;241m*\u001b[39mnary_op_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnary_op_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\differential_operators.py:28\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(fun, x):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    Returns a function which computes the gradient of `fun` with respect to\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    positional argument number `argnum`. The returned function takes the same\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    arguments as `fun`, but returns the gradient instead. The function `fun`\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    should be scalar-valued. The gradient has the same type as the argument.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m \u001b[43m_make_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[1;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  \u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39msubargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johau\\Desktop\\Approximate inference methods for BNNs\\approximate_bayesian_inference_methods\\utils.py:190\u001b[0m, in \u001b[0;36mBlackBoxVariationalInference.compute_ELBO\u001b[1;34m(self, lam)\u001b[0m\n\u001b[0;32m    187\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[batch_idx, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[batch_idx]\n\u001b[1;32m--> 190\u001b[0m     expected_log_lik_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_lik\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_samples\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: scalar\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# No mini-batching\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     expected_log_lik_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_lik(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, w_samples)   \u001b[38;5;66;03m# shape: scalar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mlog_like_NN_classification\u001b[1;34m(X, y, theta_s)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(S):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of theta_s[i]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, theta_s[i]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 49\u001b[0m     net_s \u001b[38;5;241m=\u001b[39m new_weights_in_NN(net, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(net_s(X)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#categorical log likelihood\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johau\\anaconda3\\envs\\test_env\\lib\\site-packages\\autograd\\numpy\\numpy_boxes.py:22\u001b[0m, in \u001b[0;36mArrayBox.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "#load weights\n",
    "weights = torch.load('checkpoints\\LeNet5_acc_95.12%.pth')\n",
    "theta_map = torch.cat([w.flatten() for w in weights.values()]) # flatten the weights\n",
    "theta_map = theta_map.detach().numpy()\n",
    "\n",
    "# settings\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print('Number of parameters:', num_params)\n",
    "K = 10\n",
    "P = torch.randn(num_params, K).numpy() # random matrix from normal distribution\n",
    "max_itt = 1\n",
    "step_size = 5e-2\n",
    "num_samples = 20\n",
    "batch_size = 100\n",
    "seed = 0\n",
    "verbose = True\n",
    "\n",
    "bbvi = BlackBoxVariationalInference(theta_map, P, log_prior_pdf, log_like_NN_classification, K, step_size, max_itt, num_samples, batch_size, seed, verbose)\n",
    "bbvi.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
